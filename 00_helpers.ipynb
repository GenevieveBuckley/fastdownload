{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import yaml, json\n",
    "import requests, hashlib\n",
    "import tarfile, zipfile\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.all import *\n",
    "from fastprogress.fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Config:\n",
    "    \"Setup config at `~/.fastai` unless it exists already.\"\n",
    "    config_path = Path('~/.fastai').expanduser()\n",
    "    config_file = config_path/'config.yml'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config_path.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.config_file.exists(): self.create_config()\n",
    "        self.d = self.load_config()\n",
    "\n",
    "    def __getitem__(self,k):\n",
    "        k = k.lower()\n",
    "        if k not in self.d: k = k+'_path'\n",
    "        return Path(self.d[k])\n",
    "\n",
    "    def __getattr__(self,k):\n",
    "        if k=='d': raise AttributeError\n",
    "        return self[k]\n",
    "\n",
    "    def __setitem__(self,k,v): self.d[k] = str(v)\n",
    "    def __contains__(self,k): return k in self.d\n",
    "\n",
    "    def load_config(self):\n",
    "        \"load and return config if version equals 2 in existing, else create new config.\"\n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config\n",
    "\n",
    "    def create_config(self, cfg=None):\n",
    "        \"create new config with default paths\"\n",
    "        config = {\n",
    "            'data_path':    str(self.config_path/'data'),\n",
    "            'archive_path': str(self.config_path/'archive')\n",
    "        }\n",
    "        if cfg is not None:\n",
    "            config = merge(config, cfg)\n",
    "        self.save_file(config)\n",
    "\n",
    "    def save(self): self.save_file(self.d)\n",
    "    def save_file(self, config):\n",
    "        \"save config file at default config location `~/.fastai/config.yml`.\"\n",
    "        with self.config_file.open('w') as f: yaml.dump(config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'archive_path': '/home/arora/.fastai/archive',\n",
       " 'data_path': '/home/arora/.fastai/data'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# creates config\n",
    "config = Config()\n",
    "config.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_url(url, dest, overwrite=False, pbar=None, show_progress=True, chunk_size=1024*1024,\n",
    "                timeout=4, retries=5):\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`\"\n",
    "    if os.path.exists(dest) and not overwrite: return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    # additional line to identify as a firefox browser, see fastai/#2438\n",
    "    s.headers.update({'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'})\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: pbar = progress_bar(range(file_size), leave=False, parent=pbar)\n",
    "        try:\n",
    "            if show_progress: pbar.update(0)\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            fname = url.split('/')[-1]\n",
    "            data_dir = dest.parent\n",
    "            print(f'\\n Download of {url} has failed after {retries} retries\\n'\n",
    "                f' Fix the download manually:\\n'\n",
    "                f'$ mkdir -p {data_dir}\\n'\n",
    "                f'$ cd {data_dir}\\n'\n",
    "                f'$ wget -c {url}\\n'\n",
    "                f'$ tar xf {fname}\\n'\n",
    "                f' And re-run your code once the download is successful\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_data(src, dest, force_download=False):\n",
    "    \"Download `url` to `fname`.\"\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not dest.exists() or force_download: download_url(src, dest)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def _get_dset_name(url):    \n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    for key, val in checks.items(): \n",
    "        if val[0]==url: return key \n",
    "    else: raise ValueError(f\"No dataset exists at {Path(__file__).parent/'checks.txt'} for url - {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _add_check(fpath, url):\n",
    "    \"Internal function to update the internal check file with `url` and check on `fname`.\"\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    dset_name = _get_dset_name(url)\n",
    "    checks[dset_name] = [url] + _check_file(fpath)\n",
    "    json.dump(checks, open(Path(__file__).parent/'checks.txt', 'w'), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _check_file(fpath):\n",
    "    \"internal function to get the hash of the local file at `fname`.\"\n",
    "    size = os.path.getsize(fpath)\n",
    "    with open(fpath, \"rb\") as f: hash_nb = hashlib.md5(f.read(2**20)).hexdigest()\n",
    "    return [size,hash_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def _get_check(url):\n",
    "    \"internal function to get the hash of the file at `url`.\"\n",
    "    checks = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    dset_name = _get_dset_name(url)\n",
    "    return checks.get(dset_name)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_URLs():\n",
    "    \"internal function to get the URLs from `checks.txt`\"\n",
    "    URLs = json.load(open(Path(__file__).parent/'checks.txt', 'r'))\n",
    "    return dict2obj(URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def _check_present(url_meta, fpath):\n",
    "    return [url_meta[0]] + _check_file(fpath) == url_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def file_extract(fname, dest=None):\n",
    "    \"Extract `fname` to `dest` using `tarfile` or `zipfile`.\"\n",
    "    if dest is None: dest = Path(fname).parent\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_helpers.ipynb.\n",
      "Converted 01_url.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
